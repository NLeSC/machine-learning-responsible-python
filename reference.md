---
title: Glossary
---

- **AI**:
  [Artificial intelligence](https://glosario.carpentries.org/en/#artificial_intelligence) can be used to refer to a series of existing algorithm families e.g. machine learning, reinforcement learning, deep learning and generative algorithms but also poential algorithms that can perform tasks one thought to be unique to human intelligence..

- **Algorithmic bias**:
[Algorithmic bias](https://glosario.carpentries.org/en/#algorithmic_bias) refers to bias in an algorithm, and is often particularly problematic with algorithms meant to predict human behaviour e.g. criminality or diagnose medical conditions..

- **Augmented Data**:
  Augmented data is data created by manipulation of original raw data in a dataset. In the case of images this might be something like an affine transform or a contrast enhancement, although in this example only the affine transform allows full recovery of original data, both could allow re-identification of original data. All kinds of data could be augmented to produce more training material for AI algorithms. check for Glossario link?

- **Automation Bias**:
  The tendency to trust or favor machine generated results over human ones. We may have tendencies to ignore contrary data, especially if it is human generated. Humans may tend towards low cognitive effort decisions. This issue has a longer history of research in the aviation industry, where there may be lessons to be learned.

- **Autonomy**:
 The idea that individuals should be independant in making choices and controling their future. Autonomy is a pillar of medical ethics. 

- **Beneficence**:
 Beneficence is a pillar of medical ethics which implies acting for the good or benefit of individual patients.

- **The Chinese room argument**:
  The [chinese room argument](https://plato.stanford.edu/entries/chinese-room/) is a well know philisophical argument about a computer program by John Searl interpreted by many to imply that the Turing test is inadequte to diagnose true understanding and intelligence. 

- **Conseuquentialism**:
  Consequentialism is the ethical stance that we must weight the consequences of our actions to understand their morality. Taken to a logical end this would mean actions commonly understood as morally deficient e.g. murder, might be the ethical best choices depending upon the circumstance.
  
- **Data Colonialism**:
  A description of the capture of human related and generated data on individuals  e.g. your social mediat activity by actors such as large corporations for their gain including financial and other types of gain (behavioural coercion) as a new form of colonialism. An interesting note is that citizens of post-colonial countries may be more at risk for this problem due to weaker legal enforcement via a vis big tech or even a lack of any laws on data privacy. [needs links]

- **Deontological ethics**:
  Deonological ethics could be contrasted with consequentialism. In this ethical approach actions must be weighed regardless of conseuqences or in spite of them as would be the case in the story of [(Immanuel) Kant's axe](https://www.open.edu/openlearn/history-the-arts/culture/philosophy/kants-axe).
 

- **Doctrine of Double effect**:
  This doctrine was outlined by Thomas Aquinas. It states that actions may have both an intended and unintended effect, and we are morally responsible for the intended effect so long as the unintended effect was not our goal. The classic example is one of civilian deaths when targetting combatants in a war. This is viewed as acceptable so long as it was not the goal. Can we apply such a doctrine in the case of offensive content stickers for Facebook users in Mynmar? A side effect was that they increased interaction i.e. "maximized engagement" amplifying messages of anti-Rohingya hatred. 

- **Epistemology**:
  A branch of philosophy concerned with th structure and basis of knowledge. 


- **FAT algorithm**: An algorithm with fairness, accountability and transparency. These qualities can be contrasted to those of many algorithms critical to modern life as pointed out by many scholars including Cathy O'Niel Harris in her book 'Weapons of Math Destruction' who posits it is very common for algorithmic tools to be opaque and difficult to contest. 


- **Helsinki declaration**:
  The Helinksi declaration is essentially a global guideline that is codified locally in countries to help define ethical boandaries around human medical experiments. Respect for the individual, consent and consideration for their well-being. Along with the Nuremberg code it is the backbone of the globally accepted code for ethical aspects of human research.

 
- **Hypernudge**:
  A relatively (2017) new term to descibe an intentional system of algorithmic human behavioural regulation based on data collection and interaction without explicit use of force and coercion. Depending upon the scholar defining it, these nudges may also be adaptive, changing over time as the user behaviour changes. Hypernudging is different from simple nudging, even in digital form, in that it involves personalized data driving choice environments. For example a social media platform may have a feed which offers certain choices, meant to influence behavious, prominently, and others not at all; all based on AI based profiles of the user. This can be argued implications in terms autonomy versus paternalism and non-manipulation which may imply non-malevolnce.


- **Impossibility theory**:
  The idea that in terms of predictive algorithms expect in two specific special cases you can't be fair in traditional social sense, and have the highest performace with a statistcial model. Example, a breast cancer prediction model in the US might perform best overall if it is biased against African-AMerican women (who while less likely to develop the disease, get it earlier if they do, and have a variety of different reasons for differing presentation); and this goes against our notion of fairness that the algorithm should work for everyone and not increase existing discrepancies. The special cases where impossibility theory is believed to not hold are when your algorithm is 100% accurate for all, or the distribution of an outcome is exactly equal across all groups. 


- **Justice**: 
  Justice has been a theme in philosophy and ethics since at least the time of Plato. It is often interpreted to mean individuals should be treated imparitally. Justice is a pillar of medical ethics. In terms of medical ethics justice implies treating all people fairly and in the spirit of equality. 


-**Long termism**:
  Long termism a philosophical stance popular among techno-solutionists and others in the effective altruism movement. According to this stance we must prioritize future lives, even lives in the distant future as of equal moral weight to those of lives today. The term is new, only coinced in 2017 by philosophers William MacAskill and Toby Ord, but the concept that we should think of future generations has strong roots in multiple cultures. Longg termist ideas tend to differ from traditional convential ideas on helping future generation in the scale of the future they imagine, and the type of threats and goals they ponder i.e. AI apocylism, transhumanism etc.


- **Nonmalevolence**:
Nonmalevolence is a pillar of medical ethics. It implies acting towards patients in a way to 'do no harm' or "primum, non nocere".


- **Nuremberg Code**:
  The Nuremberg code is a globally accepted code about experiments in humans. It was established in the aftermath of world war II, during which horrific and unethical experiments were carried out on prisoners.

- **Privacy**:
  Privacy implies freedom from the extraction and sharing of sensitive personal information. Most research protocols must be made to consider the privacy of participants.

- **Paternalism**:
  The attitude that some degree of coercion and manipulation of individuals by others with more power and authority is desirable. While paternalism contrasts with autonomy, it is actually implicitly a value in the way many laws are written. Lawmakers decide if you must take certain actions which may only effect yourself such as voluntary euthenasia (in some cases).


- **Protected Attribute**:
  In the lingo of machine learning fairness/bias work, this is a human attribute (often an immutable attribute of birth like 'race') that is known to cause discrimination, but already protected by anti-discrimination laws for protected classes. 

- **Researcher Positionality**:

- **Stochastic Parrot**:

- **Synthetic Data**:
  Sythnetic data is data which resembles real data but is made by processes other than simple data augmentation. In some "synthetic" datasets there may be a risk to reconstruct parts of the original data. If the data was about humans e.g. hospital records, this potentially raises privacy issues.

- **Utilitarianism**:
  Utilitarianism is an ethical stance first articulated by Jeremy Bentham. According to a utilitarianist perspective we should make the most people the most happy, and the least people unhappy and this can be determined or at least modeled with simple arithematic. Since most actions do not effect all people, we can think about stakeholders. However it should be noted that environmental degredation caused by AI production may mean every human is a stakeholder.


- **Virtue ethics**:
 

